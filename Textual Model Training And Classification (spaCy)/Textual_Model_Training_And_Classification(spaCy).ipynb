{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857002d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: {'ner': 43.953558921813965}\n",
      "Epoch 2: {'ner': 9.589602072468338}\n",
      "Epoch 3: {'ner': 9.210063129169928}\n",
      "Epoch 4: {'ner': 6.691108042699546}\n",
      "Epoch 5: {'ner': 2.5227867507614907}\n",
      "Epoch 6: {'ner': 1.2956727009249496}\n",
      "Epoch 7: {'ner': 0.923140477453687}\n",
      "Epoch 8: {'ner': 1.75360219311091}\n",
      "Epoch 9: {'ner': 3.3206068893509024}\n",
      "Epoch 10: {'ner': 1.0034120495169685}\n",
      "Epoch 11: {'ner': 0.40066619597927194}\n",
      "Epoch 12: {'ner': 0.00032825979292121823}\n",
      "Epoch 13: {'ner': 0.0005531579285755798}\n",
      "Epoch 14: {'ner': 1.3040752129192714e-05}\n",
      "Epoch 15: {'ner': 1.0126008759119838e-05}\n",
      "Модель збережено в папку 'medical_ner_model'\n",
      "\n",
      "Прогнозовані сутності:\n",
      "\n",
      "Текст: The patient was diagnosed with pneumonia.\n",
      " - Сутність: pneumonia, Мітка: DISEASE, Початок: 31, Кінець: 40\n",
      "\n",
      "Текст: He suffers from chronic asthma.\n",
      " - Сутність: chronic asthma, Мітка: DISEASE, Початок: 16, Кінець: 30\n",
      "\n",
      "Текст: She was treated for breast cancer.\n",
      " - Сутність: cancer, Мітка: DISEASE, Початок: 27, Кінець: 33\n",
      "\n",
      "Текст: The doctor confirmed a diagnosis of diabetes.\n",
      " - Сутність: diabetes, Мітка: DISEASE, Початок: 36, Кінець: 44\n",
      "\n",
      "Текст: They are treating him for hypertension.\n",
      " - Сутність: hypertension, Мітка: DISEASE, Початок: 26, Кінець: 38\n",
      "\n",
      "Текст: This is just for a test\n"
     ]
    }
   ],
   "source": [
    "# Ознайомитись з додаванням власних прикладів до моделей spaCy та компонентом для класифікації текстів.\n",
    "# (Розпізнавання сутностей та класифікація текстів за допомогою textCat)\n",
    "import spacy\n",
    "import json\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "# Зчитування даних із JSON-файлу\n",
    "with open(\"1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Підготовка у форматі (text, {\"entities\": [(start, end, label)]})\n",
    "TRAIN_DATA = [(item[\"text\"], {\"entities\": [tuple(ent) for ent in item[\"entities\"]]}) for item in raw_data]\n",
    "\n",
    "# Створення нової пустої моделі (англ)\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Додаємо NER-пайплайн (Named Entity Recognizer - модель розпізнавання іменованих сутностей)\n",
    "if \"ner\" not in nlp.pipe_names: # перевірка імен сутностей в конвеєрі \n",
    "    ner = nlp.add_pipe(\"ner\") #додаємо розпізнавальник до конвеєра\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\") #дістаємо модель з конвеєру\n",
    "\n",
    "# Додаємо нову мітку сутності\n",
    "LABEL = \"DISEASE\"\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "\n",
    "# Ініціалізація параметрів моделі\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "# Навчання моделі\n",
    "for epoch in range(15):\n",
    "    losses = {}\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    batches = minibatch(TRAIN_DATA, size=2) # для оновлення вагів моделі після обробки 1-го батчу\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations) # Приклад для навчання з пари текст-анотація\n",
    "            examples.append(example)\n",
    "            #nlp.update([example], sgd=optimizer, drop=0.25, losses=losses) # Додавання нових прикладів в модель з вимкненням drop% нейронів для запобігання перенавчанню та записом втрат\n",
    "        nlp.update(examples, sgd=optimizer, drop=0.25, losses=losses)\n",
    "    print(f\"Epoch {epoch+1}: {losses}\")\n",
    "\n",
    "# Збереження моделі\n",
    "#ner = nlp.get_pipe(\"ner\")\n",
    "#ner.to_disk(\"medical_ner_model\")\n",
    "nlp.to_disk(\"medical_ner_model\")\n",
    "print(\"Модель збережено в папку 'medical_ner_model'\")\n",
    "\n",
    "# Тестові приклади для прогнозування\n",
    "test_texts = [\n",
    "    \"The patient was diagnosed with pneumonia.\",\n",
    "    \"He suffers from chronic asthma.\",\n",
    "    \"She was treated for breast cancer.\",\n",
    "    \"The doctor confirmed a diagnosis of diabetes.\",\n",
    "    \"They are treating him for hypertension.\",\n",
    "    \"This is just for a test\"\n",
    "]\n",
    "\n",
    "# nlp = spacy.load(\"medical_ner_model\")\n",
    "\n",
    "print(\"\\nПрогнозовані сутності:\")\n",
    "for text in test_texts:\n",
    "    doc = nlp(text)\n",
    "    print(f\"\\nТекст: {text}\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - Сутність: {ent.text}, Мітка: {ent.label_}, Початок: {ent.start_char}, Кінець: {ent.end_char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2189\n",
      "Epoch 2, Loss: 0.2133\n",
      "Epoch 3, Loss: 0.2001\n",
      "Epoch 4, Loss: 0.1860\n",
      "Epoch 5, Loss: 0.1667\n",
      "Epoch 6, Loss: 0.1431\n",
      "Epoch 7, Loss: 0.1248\n",
      "Epoch 8, Loss: 0.0857\n",
      "Epoch 9, Loss: 0.0701\n",
      "Epoch 10, Loss: 0.0496\n",
      "Epoch 11, Loss: 0.0338\n",
      "Epoch 12, Loss: 0.0353\n",
      "Epoch 13, Loss: 0.0221\n",
      "Epoch 14, Loss: 0.0180\n",
      "Epoch 15, Loss: 0.0108\n",
      "\n",
      "Text: I need a technician to fix my internet.\n",
      "Predicted intent: technician_request (score: 0.302)\n",
      "\n",
      "Text: Can I get a cheaper plan?\n",
      "Predicted intent: billing_question (score: 0.835)\n",
      "\n",
      "Text: Cancel my internet service now.\n",
      "Predicted intent: cancel_service (score: 0.833)\n",
      "\n",
      "Text: Tell me about your internet plans\n",
      "Predicted intent: pricing_info (score: 0.326)\n"
     ]
    }
   ],
   "source": [
    "# Ознайомитись з додаванням власних прикладів до моделей spaCy та компонентом для класифікації текстів.\n",
    "# (Розпізнавання сутностей та класифікація текстів за допомогою textCat)\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "\n",
    "# Завантаження даних\n",
    "with open(\"2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Підготовка тренувальних прикладів\n",
    "train_data = []\n",
    "for item in data:\n",
    "    utterance = item[\"utterance\"] # Висловлювання (текст/повідомлення)\n",
    "    intent = item[\"intent\"] # Намір повідомлення / категорія (клас)\n",
    "    train_data.append((utterance, {\"cats\": {intent: 1.0}})) # Cтворємо тренувальний словник категорій для класифікації, де відповідний intent має ймовірність 1.0.\n",
    "\n",
    "# Список унікальних intent-ів\n",
    "labels = list(set(item[\"intent\"] for item in data))\n",
    "\n",
    "# Створення моделі spaCy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Додаємо TextCategorizer до конвеєра та налаштовуємо через конфіг\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "textcat.cfg[\"exclusive_classes\"] = True # тільки одна категорія може бути активною (один intent), а інші по нулям\n",
    "textcat.cfg[\"architecture\"] = \"bow\"  # сумка слів або \"simple_cnn\" (простий згортковий класифікатор)\n",
    "\n",
    "for label in labels:\n",
    "    textcat.add_label(label)\n",
    "\n",
    "\n",
    "train_examples = [Example.from_dict(nlp.make_doc(text),label) for text,label in train_data] # Приклади для ініціалізації параметрів (ваг) моделі з пари текст-анотація\n",
    "# Ініціалізація параметрів компоненту textcat\n",
    "#textcat.initialize(lambda: train_examples, nlp=nlp)\n",
    "\n",
    "# Ініціалізація параметрів моделі\n",
    "optimizer = nlp.initialize(lambda: train_examples) # spaCy використовує оптимізатор Adam (Оновлює ваги на основі середнього значення градієнтів (momentum) та середньо-квадратичного значення градієнтів)\n",
    "\n",
    "# Навчання моделі\n",
    "for epoch in range(15):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=4)\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            examples.append(Example.from_dict(doc, annotations))\n",
    "        nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {losses['textcat']:.4f}\")\n",
    "\n",
    "# Тестування\n",
    "test_texts = [\n",
    "    \"I need a technician to fix my internet.\",\n",
    "    \"Can I get a cheaper plan?\",\n",
    "    \"Cancel my internet service now.\",\n",
    "    \"Tell me about your internet plans\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    doc = nlp(text)\n",
    "    predicted = max(doc.cats, key=doc.cats.get) # Вибираємо категорію з найбільшою ймовірністю\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted intent: {predicted} (score: {doc.cats[predicted]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3284\n",
      "Epoch 2, Loss: 0.3144\n",
      "Epoch 3, Loss: 0.2940\n",
      "Epoch 4, Loss: 0.2611\n",
      "Epoch 5, Loss: 0.2216\n",
      "Epoch 6, Loss: 0.1737\n",
      "Epoch 7, Loss: 0.1279\n",
      "Epoch 8, Loss: 0.0853\n",
      "Epoch 9, Loss: 0.0507\n",
      "Epoch 10, Loss: 0.0269\n",
      "Epoch 11, Loss: 0.0146\n",
      "Epoch 12, Loss: 0.0076\n",
      "Epoch 13, Loss: 0.0029\n",
      "Epoch 14, Loss: 0.0011\n",
      "Epoch 15, Loss: 0.0005\n",
      "\n",
      "Text: I need a technician to fix my internet.\n",
      "Predicted intent: technical_issue (score: 0.255)\n",
      "\n",
      "Text: Can I get a cheaper plan?\n",
      "Predicted intent: billing_question (score: 0.928)\n",
      "\n",
      "Text: Cancel my internet service now.\n",
      "Predicted intent: cancel_service (score: 0.511)\n",
      "\n",
      "Text: Tell me about your internet plans\n",
      "Predicted intent: pricing_info (score: 0.527)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "from pathlib import Path\n",
    "\n",
    "# Завантаження даних\n",
    "with open(\"2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Підготовка тренувальних прикладів\n",
    "train_data = []\n",
    "for item in data:\n",
    "    utterance = item[\"utterance\"]  # Висловлювання (текст/повідомлення)\n",
    "    intent = item[\"intent\"]  # Намір повідомлення / категорія (клас)\n",
    "    train_data.append((utterance, {\"cats\": {intent: 1.0}}))  # Cтворюємо тренувальний словник категорій для класифікації\n",
    "\n",
    "# Список унікальних intent-ів\n",
    "labels = list(set(item[\"intent\"] for item in data))\n",
    "\n",
    "# Створення моделі spaCy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Додаємо TextCategorizer до конвеєра\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "textcat.cfg[\"exclusive_classes\"] = True  # тільки одна категорія може бути активною\n",
    "textcat.cfg[\"architecture\"] = \"simple_cnn\"  # сумка слів або \"simple_cnn\"\n",
    "\n",
    "# Додаємо категорії до текстового класифікатора\n",
    "for label in labels:\n",
    "    textcat.add_label(label)\n",
    "\n",
    "# Підготовка тренувальних прикладів\n",
    "train_examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in train_data]\n",
    "\n",
    "# Ініціалізація параметрів компоненту textcat\n",
    "textcat.initialize(lambda: train_examples, nlp=nlp)\n",
    "\n",
    "# Ініціалізація оптимізатора (розпочинаємо навчання)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Навчання моделі\n",
    "for epoch in range(15):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=4)\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            examples.append(Example.from_dict(doc, annotations))\n",
    "        # Оновлення ваг\n",
    "        nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {losses['textcat']:.4f}\")\n",
    "\n",
    "# Тестування моделі\n",
    "test_texts = [\n",
    "    \"I need a technician to fix my internet.\",\n",
    "    \"Can I get a cheaper plan?\",\n",
    "    \"Cancel my internet service now.\",\n",
    "    \"Tell me about your internet plans\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    doc = nlp(text)\n",
    "    predicted = max(doc.cats, key=doc.cats.get)  # Вибираємо категорію з найбільшою ймовірністю\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted intent: {predicted} (score: {doc.cats[predicted]:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
