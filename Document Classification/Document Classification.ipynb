{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                category                                               text  \\\n",
      "0      2433    Household  # The Silky Beans 2 KG Premium Bean Bag Filler...   \n",
      "1      2184    Household  Shilpi Wooden Partition Leave Design/Room Divi...   \n",
      "2      8036    Household  HOKIPO Cotton 46L European Pattern Wash Me Pri...   \n",
      "3     25924        Books  Long Walk to Freedom-the Autobiography of Nels...   \n",
      "4     22483        Books  Face to Face CAT Common Admission Test Section...   \n",
      "...     ...          ...                                                ...   \n",
      "3147  49624  Electronics  Kaira 9H Hardness Toughened Tempered Glass Scr...   \n",
      "3148   1604    Household  SGD Plastic Strong Folding Step Stool for Adul...   \n",
      "3149  42405  Electronics  Generic 1M ISI Marked 250V Copper Power Cable ...   \n",
      "3150   2228    Household  Fursure Upholstered 5 Seater Sofa Set (Grey) (...   \n",
      "3151  43555  Electronics  HP Pavilion AIO 24–q274in 23.8-inch All-in-One...   \n",
      "\n",
      "                                          Clean Comment  \n",
      "0     the silky bean 2 kg premium bean bag filler re...  \n",
      "1     shilpi wooden partition leave design room divi...  \n",
      "2     hokipo cotton 46l european pattern wash me pri...  \n",
      "3     long walk to freedom the autobiography of nels...  \n",
      "4     face to face cat common admission test section...  \n",
      "...                                                 ...  \n",
      "3147  kaira 9h hardness toughen tempered glass scree...  \n",
      "3148  sgd plastic strong folding step stool for adul...  \n",
      "3149  generic 1m isi mark 250v copper power cable co...  \n",
      "3150  fursure upholster 5 seater sofa set grey 3 1 1...  \n",
      "3151  hp pavilion aio 24 q274in 23 8 inch all in one...  \n",
      "\n",
      "[3152 rows x 4 columns]\n",
      "\n",
      "\n",
      "Search & delete null rows\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0                  3152 non-null   object\n",
      " 1   category       3152 non-null   object\n",
      " 2   text           3152 non-null   object\n",
      " 3   Clean Comment  3152 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 98.6+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0                  3152 non-null   object\n",
      " 1   category       3152 non-null   object\n",
      " 2   text           3152 non-null   object\n",
      " 3   Clean Comment  3152 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 98.6+ KB\n",
      "\n",
      "\n",
      "Train corpus:   (2206,)\n",
      "Test corpus:   (946,)\n",
      "Word2Vec model:> Train features shape: (2206, 100)  Test features shape: (946, 100) \n",
      "\n",
      "LogReg_Word2Vec: 0.9238\n",
      "LogReg_TFIDF: 0.9116\n",
      "RF_Word2Vec: 0.9084\n",
      "RF_TFIDF: 0.8454\n",
      "LogReg_Word2Vec_GridSearch: 0.9238\n",
      "LogReg_TFIDF_GridSearch: 0.9266\n",
      "RF_Word2Vec_GridSearch: 0.9229\n",
      "RF_TFIDF_GridSearch: 0.8849\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Функція для перетворення POS тегів на формат, який використовує WordNetLemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "csvfile = open (\"ecommerceDataset3.csv\", encoding ='utf-8', mode = \"r\", newline='')\n",
    "reader = csv.DictReader(csvfile)\n",
    "\n",
    "corpus = []\n",
    "category = []\n",
    "ids = []\n",
    "for row in reader:\n",
    "    ids.append(row[''])\n",
    "    category.append(row['category'])\n",
    "    corpus.append(row['text'])\n",
    "\n",
    "data_df = pd.DataFrame({'': ids, 'category': category, 'text': corpus})\n",
    "data_df = data_df[:10000]\n",
    "\n",
    "# Імпортуємо лематизатор\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()  # Ініціалізація лематизатора\n",
    "\n",
    "p_corpus = []\n",
    "\n",
    "for doc in corpus:\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^a-zA-Z0-9]', ' ', doc)\n",
    "\n",
    "    # Токенізація та POS-тегування\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Лематизація з врахуванням частин мови\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)  # Перетворення на формат для WordNet\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token, wordnet_pos))\n",
    "\n",
    "    doc = ' '.join(lemmatized_tokens)\n",
    "    p_corpus.append(doc)\n",
    "\n",
    "data_df['Clean Comment'] = p_corpus\n",
    "print(data_df)\n",
    "\n",
    "# Очищення даних\n",
    "print(\"\\n\\nSearch & delete null rows\\n\")\n",
    "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
    "# data_df.info()\n",
    "data_df = data_df.dropna().reset_index(drop=True)\n",
    "#print()\n",
    "data_df.info()\n",
    "\n",
    "# Розбиття на train і test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_corpus, test_corpus, train_category, test_category = train_test_split(np.array(data_df['Clean Comment']),\n",
    "   np.array(data_df['category']), test_size=0.3, random_state=0)\n",
    "print(\"\\n\\nTrain corpus:   {}\".format(train_corpus.shape))\n",
    "print(\"Test corpus:   {}\".format(test_corpus.shape))\n",
    "\n",
    "# Токенізація\n",
    "tokenized_train = [wpt.tokenize(text) for text in train_corpus]\n",
    "tokenized_test = [wpt.tokenize(text) for text in test_corpus]\n",
    "\n",
    "# Створення Word2Vec моделі\n",
    "from gensim.models import word2vec\n",
    "w2v_model  = word2vec.Word2Vec(tokenized_train, vector_size=100, window=100, min_count=2,\n",
    "    sample=1e-3, sg=1, workers=10)\n",
    "\n",
    "# Векторизація документів\n",
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)  # всі слова в моделі\n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "        return feature_vector\n",
    "    features = [average_word_vectors(tokenized_sentence, model,\n",
    "                vocabulary, num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "avg_wv_train_features = document_vectorizer(corpus=tokenized_train,\n",
    "    model=w2v_model, num_features=100)\n",
    "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "    num_features=100)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape,' Test features shape:', avg_wv_test_features.shape, '\\n')\n",
    "\n",
    "# Логістична регресія\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2', max_iter=2000, C=1, random_state=0)\n",
    "\n",
    "lr.fit(avg_wv_train_features, train_category)\n",
    "\n",
    "lr_bow_w2v_scores = cross_val_score(lr, avg_wv_train_features, train_category, cv=5)\n",
    "print('LogReg w2v Accuracy (5-fold):', lr_bow_w2v_scores)\n",
    "\n",
    "lr_bow_w2v_mean_score = np.mean(lr_bow_w2v_scores)\n",
    "print('LogReg Mean w2v Accuracy:', lr_bow_w2v_mean_score)\n",
    "\n",
    "lr_bow_test_score = lr.score(avg_wv_test_features, test_category)\n",
    "print('LogReg W2V Test Accuracy:', lr_bow_test_score)\n",
    "\n",
    "# Випадковий ліс\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "\n",
    "rfc.fit(avg_wv_train_features, train_category)\n",
    "\n",
    "rfc_bow_w2v_scores = cross_val_score(rfc, avg_wv_train_features, train_category, cv=5)\n",
    "print('RF w2v Accuracy (5-fold):', rfc_bow_w2v_scores)\n",
    "\n",
    "rfc_bow_w2v_mean_score = np.mean(rfc_bow_w2v_scores)\n",
    "print('RF Mean w2v Accuracy:', rfc_bow_w2v_mean_score)\n",
    "\n",
    "rfc_bow_test_score = rfc.score(avg_wv_test_features, test_category)\n",
    "print('RF W2V Test Accuracy:', rfc_bow_test_score)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Векторизація\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "train_tfidf_features = vectorizer.fit_transform(train_corpus)\n",
    "test_tfidf_features = vectorizer.transform(test_corpus)\n",
    "\n",
    "# Логістична регресія (TF-IDF)\n",
    "lr_tfidf = LogisticRegression(penalty='l2', max_iter=2000, C=1, random_state=0)\n",
    "lr_tfidf.fit(train_tfidf_features, train_category)\n",
    "lr_tfidf_scores = cross_val_score(lr_tfidf, train_tfidf_features, train_category, cv=5)\n",
    "print('LogReg TF-IDF Accuracy (5-fold):', lr_tfidf_scores)\n",
    "lr_tfidf_mean_score = np.mean(lr_tfidf_scores)\n",
    "print('LogReg TF-IDF Mean Accuracy:', lr_tfidf_mean_score)\n",
    "lr_tfidf_test_score = lr_tfidf.score(test_tfidf_features, test_category)\n",
    "print('LogReg TF-IDF Test Accuracy:', lr_tfidf_test_score)\n",
    "\n",
    "# Випадковий ліс (TF-IDF)\n",
    "rfc_tfidf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rfc_tfidf.fit(train_tfidf_features, train_category)\n",
    "rfc_tfidf_scores = cross_val_score(rfc_tfidf, train_tfidf_features, train_category, cv=5)\n",
    "print('RF TF-IDF Accuracy (5-fold):', rfc_tfidf_scores)\n",
    "rfc_tfidf_mean_score = np.mean(rfc_tfidf_scores)\n",
    "print('RF TF-IDF Mean Accuracy:', rfc_tfidf_mean_score)\n",
    "rfc_tfidf_test_score = rfc_tfidf.score(test_tfidf_features, test_category)\n",
    "print('RF TF-IDF Test Accuracy:', rfc_tfidf_test_score)\n",
    "\n",
    "# GridSearchCV для налаштування гіперпараметрів\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {\"C\": np.logspace(-3, 3, 7), \"penalty\": [\"l2\"]}\n",
    "logreg = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# GridSearchCV для логістичної регресії (Word2Vec)\n",
    "logreg_cv = GridSearchCV(logreg, grid, cv=5)\n",
    "logreg_cv.fit(avg_wv_train_features, train_category)\n",
    "print(\"GS LogReg W2V Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\n",
    "print(\"GS LogReg W2V Accuracy: \", logreg_cv.best_score_)\n",
    "\n",
    "# GridSearchCV для логістичної регресії (TF-IDF)\n",
    "logreg_cv_tfidf = GridSearchCV(logreg, grid, cv=5)\n",
    "logreg_cv_tfidf.fit(train_tfidf_features, train_category)\n",
    "print(\"GS LogReg TF-IDF Tuned hyperparameters (best parameters): \", logreg_cv_tfidf.best_params_)\n",
    "print(\"GS LogReg TF-IDF Accuracy: \", logreg_cv_tfidf.best_score_)\n",
    "\n",
    "# GridSearchCV для випадкових лісів (Word2Vec)\n",
    "rfc_param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "rfc_cv_w2v = GridSearchCV(RandomForestClassifier(random_state=0), rfc_param_grid, cv=5)\n",
    "rfc_cv_w2v.fit(avg_wv_train_features, train_category)\n",
    "print(\"GS RF W2V Tuned hyperparameters (best parameters): \", rfc_cv_w2v.best_params_)\n",
    "print(\"GS RF W2V Accuracy: \", rfc_cv_w2v.best_score_)\n",
    "\n",
    "# GridSearchCV для випадкових лісів (TF-IDF)\n",
    "rfc_cv_tfidf = GridSearchCV(RandomForestClassifier(random_state=0), rfc_param_grid, cv=5)\n",
    "rfc_cv_tfidf.fit(train_tfidf_features, train_category)\n",
    "print(\"GS RF TF-IDF Tuned hyperparameters (best parameters): \", rfc_cv_tfidf.best_params_)\n",
    "print(\"GS RF TF-IDF Accuracy: \", rfc_cv_tfidf.best_score_)\n",
    "\n",
    "# Порівняння точності\n",
    "# results = {\n",
    "#     'LogReg_Word2Vec': lr_bow_w2v_mean_score,\n",
    "#     'LogReg_TFIDF': lr_tfidf_mean_score,\n",
    "#     'RF_Word2Vec': rfc_bow_w2v_mean_score,\n",
    "#     'RF_TFIDF': rfc_tfidf_mean_score,\n",
    "#     'LogReg_Word2Vec_GridSearch': logreg_cv.best_score_,\n",
    "#     'LogReg_TFIDF_GridSearch': logreg_cv_tfidf.best_score_,\n",
    "#     'RF_Word2Vec_GridSearch': rfc_cv_w2v.best_score_,\n",
    "#     'RF_TFIDF_GridSearch': rfc_cv_tfidf.best_score_\n",
    "# }\n",
    "\n",
    "# for model, score in results.items():\n",
    "#     print(f'{model}: {score:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
